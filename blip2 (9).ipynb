{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune BLIP using Hugging Face `transformers` and `datasets` ü§ó\n\nThis tutorial is largely based from the [GiT tutorial](https://colab.research.google.com/drive/1HLxgrG7xZJ9FvXckNG61J72FkyrbqKAA?usp=sharing) on how to fine-tune GiT on a custom image captioning dataset. Here we will use a dummy dataset of [football players](https://huggingface.co/datasets/ybelkada/football-dataset) ‚öΩ that is uploaded on the Hub. The images have been manually selected together with the captions. \nCheck the ü§ó [documentation](https://huggingface.co/docs/datasets/image_dataset) on how to create and upload your own image-text dataset.","metadata":{"id":"prwiIBb1lin_"}},{"cell_type":"code","source":"# install requirements\nimport sys\n\n!pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n# !git clone https://github.com/salesforce/BLIP\n!pip install pycocotools pycocoevalcap","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:03:25.832854Z","iopub.execute_input":"2023-04-14T20:03:25.833243Z","iopub.status.idle":"2023-04-14T20:04:38.723509Z","shell.execute_reply.started":"2023-04-14T20:03:25.833188Z","shell.execute_reply":"2023-04-14T20:04:38.722258Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers==4.15.0\n  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting timm==0.4.12\n  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fairscale==0.4.4\n  Downloading fairscale-0.4.4.tar.gz (235 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting sacremoses\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (3.9.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (0.13.3)\nCollecting tokenizers<0.11,>=0.10.1\n  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (2.28.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (4.11.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (6.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (23.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (4.64.1)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (0.14.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.15.0) (3.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0) (2.1.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.15.0) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.15.0) (8.1.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.15.0) (1.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.12) (9.4.0)\nBuilding wheels for collected packages: fairscale, sacremoses\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292837 sha256=535e5c322a1e9ac9852fabdc4a7a09eb1b76d6633e6db0bb403dd50303c0185b\n  Stored in directory: /root/.cache/pip/wheels/51/77/ae/9576097dae2412e61bc5435e3df7ab24df98e9e43ff6d2c07d\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e64ec4e3ffdd254c0d97b8d45f85d86a367d167f901ff0c0aa631fd83e558b7b\n  Stored in directory: /root/.cache/pip/wheels/5b/e0/77/05245143a5b31f65af6a21f7afd3219e9fa4896f918af45677\nSuccessfully built fairscale sacremoses\nInstalling collected packages: tokenizers, fairscale, timm, sacremoses, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.2\n    Uninstalling tokenizers-0.13.2:\n      Successfully uninstalled tokenizers-0.13.2\n  Attempting uninstall: timm\n    Found existing installation: timm 0.6.13\n    Uninstalling timm-0.6.13:\n      Successfully uninstalled timm-0.6.13\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.27.4\n    Uninstalling transformers-4.27.4:\n      Successfully uninstalled transformers-4.27.4\nSuccessfully installed fairscale-0.4.4 sacremoses-0.0.53 timm-0.4.12 tokenizers-0.10.3 transformers-4.15.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pycocotools\n  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting pycocoevalcap\n  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (3.5.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pycocotools) (1.21.6)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (23.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (4.38.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (9.4.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools) (4.4.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nBuilding wheels for collected packages: pycocotools\n  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp37-cp37m-linux_x86_64.whl size=373915 sha256=d03e68dbe0bb85b0fa1a1de691346348ea4f6853dd55a32443997ab4c1aa140a\n  Stored in directory: /root/.cache/pip/wheels/d0/90/d4/e9ae0a3cdbd8e0cddf6b5fe8c31774fb9bd0ae4e9754fb2314\nSuccessfully built pycocotools\nInstalling collected packages: pycocotools, pycocoevalcap\nSuccessfully installed pycocoevalcap-1.2 pycocotools-2.0.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# ! wget https://github.com/salesforce/BLIP/archive/refs/heads/main.zip\n# ! unzip main.zip\n# ! cp -r BLIP-main/* /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:04:38.726545Z","iopub.execute_input":"2023-04-14T20:04:38.726846Z","iopub.status.idle":"2023-04-14T20:04:38.736424Z","shell.execute_reply.started":"2023-04-14T20:04:38.726815Z","shell.execute_reply":"2023-04-14T20:04:38.732279Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# %cd /kaggle/working\n# ! git clone https://github.com/MahmoudQaid/Blip\n# %cd Blip","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:04:38.738037Z","iopub.execute_input":"2023-04-14T20:04:38.738482Z","iopub.status.idle":"2023-04-14T20:04:38.745737Z","shell.execute_reply.started":"2023-04-14T20:04:38.738444Z","shell.execute_reply":"2023-04-14T20:04:38.744762Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:04:38.748864Z","iopub.execute_input":"2023-04-14T20:04:38.749633Z","iopub.status.idle":"2023-04-14T20:04:38.834613Z","shell.execute_reply.started":"2023-04-14T20:04:38.749593Z","shell.execute_reply":"2023-04-14T20:04:38.833644Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# train_caption.py","metadata":{}},{"cell_type":"code","source":"def u():\n    %cd /kaggle/working\n    ! rm -r Blip\n    ! git clone https://github.com/MahmoudQaid/Blip\n    %cd Blip\nu()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:06:34.531751Z","iopub.execute_input":"2023-04-14T20:06:34.532464Z","iopub.status.idle":"2023-04-14T20:06:37.247017Z","shell.execute_reply.started":"2023-04-14T20:06:34.532423Z","shell.execute_reply":"2023-04-14T20:06:37.245696Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'Blip'...\nremote: Enumerating objects: 162, done.\u001b[K\nremote: Counting objects: 100% (162/162), done.\u001b[K\nremote: Compressing objects: 100% (135/135), done.\u001b[K\nremote: Total 162 (delta 91), reused 69 (delta 25), pack-reused 0\u001b[K\nReceiving objects: 100% (162/162), 6.97 MiB | 23.17 MiB/s, done.\nResolving deltas: 100% (91/91), done.\n/kaggle/working/Blip\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import defaultdict\nimport json,string,re\n\nmasked_word=['is','at','for','am','are','and','or','but','of']\npattern=r'\\b('+'|'.join(masked_word)+r')\\b'\n\ndef load_doc_karpathy(json_file_path,pattern=None,lower=False):\n    # this function return dictionary \n    with open(json_file_path,'r') as file:\n        data=json.loads(file.read())\n    dict_data=defaultdict(list)\n    for example in data['images']:\n        temp=[]\n        \n        for sentence in example['sentences']:\n            cap=sentence['raw']\n            if lower:\n                cap=cap.lower()\n                \n            cap=cap.translate(str.maketrans('','',string.punctuation))\n            \n            if pattern is not None:\n                cap=re.sub(pattern,'',cap)\n            cap=' '.join(cap.split())\n            if example['split']=='train':\n                dict_data[example['split']].append({'caption':cap,\n                                                    'image': example['filename'],\n                                                    'image_id': example['imgid']})\n            else:\n                temp.append(cap)\n                \n        if example['split']!='train':\n            dict_data[example['split']].append({'caption':temp,\n                                                'image': example['filename']\n                                             })\n            \n# {'train':[{'image':'name.jpg','image_id':img_id,'caption':cap}],'test':[{'image':'name.jpg','captions':[cap1,...,cap5]}],'val':[{'image':'name.jpg','captions':[cap1,...,cap5]}]}  \n    return dict_data\n\n\nKARPATHY_DATA=load_doc_karpathy(json_file_path='/kaggle/input/karpathy-splits/dataset_flickr8k.json',pattern=pattern,lower=True)   \n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:14:25.801749Z","iopub.execute_input":"2023-04-14T20:14:25.802937Z","iopub.status.idle":"2023-04-14T20:14:26.302220Z","shell.execute_reply.started":"2023-04-14T20:14:25.802893Z","shell.execute_reply":"2023-04-14T20:14:26.301148Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def prepare_ref_caps_for_evaluate(list_dict,output_json_path):\n    references={'annotations':[],\n                'images':[]}\n    j=0\n    for i,d in enumerate(list_dict):\n        for cap in d['caption']:\n            references['annotations'].append({'image_id':d['image'],'caption':cap,'id':j})\n            j+=1\n        references['images'].append({'id':d['image']})\n    json.dump(references,open(output_json_path,'w'))\n    \n    return references\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:04:42.613693Z","iopub.execute_input":"2023-04-14T20:04:42.614416Z","iopub.status.idle":"2023-04-14T20:04:42.622866Z","shell.execute_reply.started":"2023-04-14T20:04:42.614369Z","shell.execute_reply":"2023-04-14T20:04:42.621289Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len(KARPATHY_DATA['train'])","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:04:42.624731Z","iopub.execute_input":"2023-04-14T20:04:42.625141Z","iopub.status.idle":"2023-04-14T20:04:42.636729Z","shell.execute_reply.started":"2023-04-14T20:04:42.625104Z","shell.execute_reply":"2023-04-14T20:04:42.635471Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"30000"},"metadata":{}}]},{"cell_type":"code","source":"# New=defaultdict(list)\n# for i in KARPATHY_DATA:\n#     for j,d in enumerate(KARPATHY_DATA[i]):\n#         if j ==100:\n#             break\n#         New[i].append(d)\n# # d\n# KARPATHY_DATA=dict(New)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:04:42.638528Z","iopub.execute_input":"2023-04-14T20:04:42.639010Z","iopub.status.idle":"2023-04-14T20:04:42.657451Z","shell.execute_reply.started":"2023-04-14T20:04:42.638974Z","shell.execute_reply":"2023-04-14T20:04:42.656270Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# ! mkdir annotation\n# json.dump(KARPATHY_DATA['val'],open('annotation/coco_karpathy_val_gt.json','w'))\n# json.dump(KARPATHY_DATA['test'],open('annotation/coco_karpathy_test_gt.json','w'))\n# json.dump(KARPATHY_DATA['train'],open('annotation/coco_karpathy_train_gt.json','w'))","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:04:42.662616Z","iopub.execute_input":"2023-04-14T20:04:42.662972Z","iopub.status.idle":"2023-04-14T20:04:42.668260Z","shell.execute_reply.started":"2023-04-14T20:04:42.662946Z","shell.execute_reply":"2023-04-14T20:04:42.666851Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# ! pip install vit_pytorch\n# import torch\n# from vit_pytorch.vit import ViT\n\n# v = ViT(\n#     image_size = 256,\n#     patch_size = 32,\n#     num_classes = 1000,\n#     dim = 1024,\n#     depth = 6,\n#     heads = 16,\n#     mlp_dim = 2048,\n#     dropout = 0.1,\n#     emb_dropout = 0.1\n# )\n\n# # forward pass now returns predictions and the representation of the final layer\n# predictions = v(torch.randn(1, 3, 256, 256))\n# predictions.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:04:42.669915Z","iopub.execute_input":"2023-04-14T20:04:42.671082Z","iopub.status.idle":"2023-04-14T20:04:42.678355Z","shell.execute_reply.started":"2023-04-14T20:04:42.671011Z","shell.execute_reply":"2023-04-14T20:04:42.677361Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model=None","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:14:26.943003Z","iopub.execute_input":"2023-04-14T20:14:26.943358Z","iopub.status.idle":"2023-04-14T20:14:26.948266Z","shell.execute_reply.started":"2023-04-14T20:14:26.943326Z","shell.execute_reply":"2023-04-14T20:14:26.947118Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"'''\n * Copyright (c) 2022, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n'''\nimport argparse\nimport os\nimport ruamel_yaml as yaml\nimport numpy as np\nimport random\nimport time\nimport datetime\nimport json\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader\n\nfrom models.blip import blip_decoder\nimport utils\nfrom utils import cosine_lr_schedule\nfrom data import create_dataset, create_sampler, create_loader\nfrom data.utils import save_result, coco_caption_eval\n\n\n# dist.init_process_group(backend='nccl',world_size=1, init_method='env://',rank=0)\n\n\ndef train(model, data_loader, optimizer, epoch, device):\n    # train\n    model.train()  \n    \n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter('loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n    header = 'Train Caption Epoch: [{}]'.format(epoch)\n    print_freq = 50\n\n    for i, (image, caption, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n        image = image.to(device)       \n        \n        loss = model(image, caption)      \n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()    \n        \n        metric_logger.update(loss=loss.item())\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger.global_avg())     \n    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}  \n\n\n@torch.no_grad()\ndef evaluate(model, data_loader, device, config):\n    # evaluate\n    model.eval() \n    \n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Caption generation:'\n    print_freq = 10\n\n    result = []\n    for image, image_id in metric_logger.log_every(data_loader, print_freq, header): \n        \n        image = image.to(device)       \n        \n        captions = model.generate(image, sample=False, num_beams=config['num_beams'], max_length=config['max_length'], \n                                  min_length=config['min_length'])\n        \n        for caption, img_id in zip(captions, image_id):\n#             result.append({\"image_id\": img_id.item(), \"caption\": caption})\n            result.append({\"image_id\": img_id, \"caption\": caption})\n  \n    return result\n\n\ndef main(args, config,model=None):\n    utils.init_distributed_mode(args)    \n    \n    device = torch.device(args.device)\n\n    # fix the seed for reproducibility\n    seed = args.seed + utils.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    cudnn.benchmark = True\n\n    #### Dataset #### \n    print(\"Creating captioning dataset\")\n    train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config,annot_dict=KARPATHY_DATA)  \n#     train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config)  \n\n    if args.distributed:\n        num_tasks = utils.get_world_size()\n        global_rank = utils.get_rank()            \n        samplers = create_sampler([train_dataset,val_dataset,test_dataset], [True,False,False], num_tasks, global_rank)         \n    else:\n        samplers = [None, None, None]\n    \n    train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\n                                                          batch_size=[config['batch_size']]*3,num_workers=[4,4,4],\n                                                          is_trains=[True, False, False], collate_fns=[None,None,None])         \n\n    start_epoch=0\n    max_epoch=config['max_epoch']\n    #### Model #### \n    print(\"Creating model\")\n    if config['load_pretrained'] and config['pretrained']:\n        checkpoint=torch.load(config['pretrained'])\n        model = blip_decoder(pretrained=checkpoint['model'], image_size=config['image_size'], vit=config['vit'], \n                               vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], \n                               prompt=config['prompt'])\n        max_epoch=checkpoint['epoch']\n\n        start_epoch=checkpoint['epoch']+1\n        max_epoch=start_epoch+config['max_epoch']\n#         model.load_state_dict(checkpoint['model'])\n    elif model is None:\n        model = blip_decoder(pretrained='', image_size=config['image_size'], vit=config['vit'], \n                               vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], \n                               prompt=config['prompt'])\n        \n    for param in model.visual_encoder.parameters():\n        param.requires_grad=False\n\n    model = model.to(device)   \n    model_without_ddp = model\n    if args.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n        model_without_ddp = model.module \n        \n    optimizer = torch.optim.AdamW(params=model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay'])\n    if config['load_pretrained'] and config['pretrained']:\n        optimizer.load_state_dict(checkpoint['optimizer'])  \n        \n    best = 0\n    best_epoch = 0\n\n    print(\"Start training\")\n    start_time = time.time()    \n    for epoch in range(start_epoch, max_epoch):\n        if not args.evaluate:        \n            if args.distributed:\n                train_loader.sampler.set_epoch(epoch)\n                \n            cosine_lr_schedule(optimizer, epoch, config['max_epoch'], config['init_lr'], config['min_lr'])\n            train_stats = train(model, train_loader, optimizer, epoch, device) \n        val_result = evaluate(model_without_ddp, val_loader, device, config)\n        val_result_file = save_result(val_result, args.result_dir, 'val_epoch%d'%epoch, remove_duplicate='image_id')        \n    \n        test_result = evaluate(model_without_ddp, test_loader, device, config)  \n        test_result_file = save_result(test_result, args.result_dir, 'test_epoch%d'%epoch, remove_duplicate='image_id')  \n        \n        if utils.is_main_process():\n            prepare_ref_caps_for_evaluate(KARPATHY_DATA['val'],config['ann_root']+'/coco_karpathy_val_gt.json')\n            prepare_ref_caps_for_evaluate(KARPATHY_DATA['test'],config['ann_root']+'/coco_karpathy_test_gt.json')\n            coco_val = coco_caption_eval(config['ann_root'],val_result_file,'val')\n            coco_test = coco_caption_eval(config['ann_root'],test_result_file,'test')\n            if args.evaluate:            \n                log_stats = {**{f'val_{k}': v for k, v in coco_val.eval.items()},\n                             **{f'test_{k}': v for k, v in coco_test.eval.items()},                       \n                            }\n                with open(os.path.join(args.output_dir, \"evaluate.txt\"),\"a\") as f:\n                    f.write(json.dumps(log_stats) + \"\\n\")                   \n            else:             \n                save_obj = {\n                    'model': model_without_ddp.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'config': config,\n                    'epoch': epoch,\n                }\n\n                if coco_val.eval['CIDEr'] + coco_val.eval['Bleu_4'] > best:\n                    best = coco_val.eval['CIDEr'] + coco_val.eval['Bleu_4']\n                    best_epoch = epoch                \n                    torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_best.pth')) \n                    \n                log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n                             **{f'val_{k}': v for k, v in coco_val.eval.items()},\n                             **{f'test_{k}': v for k, v in coco_test.eval.items()},                       \n                             'epoch': epoch,\n                             'best_epoch': best_epoch,\n                            }\n                with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n                    f.write(json.dumps(log_stats) + \"\\n\")     \n                    \n        if args.evaluate: \n            break\n#         dist.barrier()     \n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Training time {}'.format(total_time_str)) \n    return model\nclass Args(dict):\n    __setattr__ = dict.__setitem__\n    __getattr__ = dict.__getitem__\n\nargs = {\n    'config':'./configs/caption_coco.yaml',\n    'output_dir':'output/Caption_coco' ,\n    'evaluate':False ,\n    'device':'cuda',\n    'seed':42,\n    'world_size':1,  \n    'dist_url':'env://',\n    'distributed':False\n}\nargs = Args(args) # dict2object\n\nif __name__ == '__main__':\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument('--config', default='./configs/caption_coco.yaml')\n#     parser.add_argument('--output_dir', default='output/Caption_coco')        \n#     parser.add_argument('--evaluate', action='store_true')    \n#     parser.add_argument('--device', default='cuda')\n#     parser.add_argument('--seed', default=42, type=int)\n#     parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')    \n#     parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n#     parser.add_argument('--distributed', default=True, type=bool)\n#     args = parser.parse_args()\n\n#     config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n    config={\n     'image_root': '/kaggle/input/flickr8k/Images',\n#      'image_root':'/kaggle/input/coco2014',\n     'ann_root': 'annotations',\n     'coco_gt_root': 'annotation/coco_gt',\n#      'pretrained':''\n#      'pretrained': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth', # vit: base\n#      'pretrained': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth',  # vit: large\n     'pretrained':'/kaggle/input/blip-temp/Blip/output/Caption_coco/checkpoint_best.pth',\n     'vit': 'base',\n#      'vit': 'large',\n     'vit_grad_ckpt': False,\n     'vit_ckpt_layer': 0,\n     'batch_size': 16,\n     'init_lr': 1e-05,\n     'image_size': 384,\n     'max_length': 20,\n     'min_length': 5,\n     'num_beams': 3,\n     'prompt': 'a picture of ',\n     'weight_decay': 0.05,\n     'min_lr': 0,\n     'max_epoch': 2,\n     'load_pretrained':True\n    }\n    args.result_dir = os.path.join(args.output_dir, 'result')\n\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n    Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n    Path(config['ann_root']).mkdir(parents=True, exist_ok=True)\n    \n    yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))    \n\n    model=main(args,config,model)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:14:44.529778Z","iopub.execute_input":"2023-04-14T20:14:44.530241Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Not using distributed mode\nCreating captioning dataset\nCreating model\nStart training\nTrain Caption Epoch: [1]  [   0/1875]  eta: 0:55:57  lr: 0.000005  loss: 3.1561  time: 1.7905  data: 1.1600  max mem: 9722\nTrain Caption Epoch: [1]  [  50/1875]  eta: 0:16:16  lr: 0.000005  loss: 3.0375  time: 0.5154  data: 0.0007  max mem: 10426\nTrain Caption Epoch: [1]  [ 100/1875]  eta: 0:15:19  lr: 0.000005  loss: 3.2467  time: 0.5029  data: 0.0002  max mem: 10426\nTrain Caption Epoch: [1]  [ 150/1875]  eta: 0:14:45  lr: 0.000005  loss: 3.0145  time: 0.5015  data: 0.0002  max mem: 10591\nTrain Caption Epoch: [1]  [ 200/1875]  eta: 0:14:17  lr: 0.000005  loss: 2.8578  time: 0.5004  data: 0.0001  max mem: 10591\nTrain Caption Epoch: [1]  [ 250/1875]  eta: 0:13:48  lr: 0.000005  loss: 2.9157  time: 0.5039  data: 0.0002  max mem: 10591\nTrain Caption Epoch: [1]  [ 300/1875]  eta: 0:13:20  lr: 0.000005  loss: 3.0737  time: 0.5050  data: 0.0004  max mem: 10591\nTrain Caption Epoch: [1]  [ 350/1875]  eta: 0:12:53  lr: 0.000005  loss: 3.1479  time: 0.5014  data: 0.0003  max mem: 10591\nTrain Caption Epoch: [1]  [ 400/1875]  eta: 0:12:27  lr: 0.000005  loss: 3.0204  time: 0.5030  data: 0.0002  max mem: 10591\nTrain Caption Epoch: [1]  [ 450/1875]  eta: 0:12:01  lr: 0.000005  loss: 2.8691  time: 0.5015  data: 0.0002  max mem: 10591\nTrain Caption Epoch: [1]  [ 500/1875]  eta: 0:11:35  lr: 0.000005  loss: 2.9426  time: 0.5106  data: 0.0002  max mem: 10591\nTrain Caption Epoch: [1]  [ 550/1875]  eta: 0:11:10  lr: 0.000005  loss: 2.8134  time: 0.5086  data: 0.0002  max mem: 10591\nTrain Caption Epoch: [1]  [ 600/1875]  eta: 0:10:44  lr: 0.000005  loss: 3.0151  time: 0.4997  data: 0.0002  max mem: 10591\n","output_type":"stream"}]},{"cell_type":"code","source":"# from torchsummary import summary\n# summary(model,)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.015284Z","iopub.status.idle":"2023-04-14T20:07:37.015672Z","shell.execute_reply.started":"2023-04-14T20:07:37.015497Z","shell.execute_reply":"2023-04-14T20:07:37.015517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert(False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.018110Z","iopub.status.idle":"2023-04-14T20:07:37.018627Z","shell.execute_reply.started":"2023-04-14T20:07:37.018372Z","shell.execute_reply":"2023-04-14T20:07:37.018399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nglob('output/Caption_coco/result/*')","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.020386Z","iopub.status.idle":"2023-04-14T20:07:37.020886Z","shell.execute_reply.started":"2023-04-14T20:07:37.020619Z","shell.execute_reply":"2023-04-14T20:07:37.020645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# chpt['model'].keys()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.023055Z","iopub.status.idle":"2023-04-14T20:07:37.024239Z","shell.execute_reply.started":"2023-04-14T20:07:37.023950Z","shell.execute_reply":"2023-04-14T20:07:37.023977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visual_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.025574Z","iopub.status.idle":"2023-04-14T20:07:37.026446Z","shell.execute_reply.started":"2023-04-14T20:07:37.026162Z","shell.execute_reply":"2023-04-14T20:07:37.026188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch \nfrom torchvision.datasets.utils import download_url\ndownload_url('https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth','/kaggle/working')\nchpt=torch.load('/kaggle/working/model_large_caption.pth')\nvisual_dict={}\nfor i in chpt['model']:\n    if i.split('.')[0]=='visual_encoder':\n        visual_dict[i[len('visual_encoder.'):]]=chpt['model'][i]\nmodel.visual_encoder.load_state_dict(visual_dict)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.028051Z","iopub.status.idle":"2023-04-14T20:07:37.028921Z","shell.execute_reply.started":"2023-04-14T20:07:37.028661Z","shell.execute_reply":"2023-04-14T20:07:37.028687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download_url('https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth','/kaggle/working')\n# chpt2=torch.load('/kaggle/working/model_base_caption_capfilt_large.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.030662Z","iopub.status.idle":"2023-04-14T20:07:37.031346Z","shell.execute_reply.started":"2023-04-14T20:07:37.031063Z","shell.execute_reply":"2023-04-14T20:07:37.031091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # len(chpt2['model'].keys()),len(chpt['model'].keys())\n# for i,key in enumerate(list(chpt['model'].keys())):\n#     if key not in list(chpt2['model'].keys()):\n#         print(i,\" : \",key)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.038415Z","iopub.status.idle":"2023-04-14T20:07:37.039162Z","shell.execute_reply.started":"2023-04-14T20:07:37.038908Z","shell.execute_reply":"2023-04-14T20:07:37.038934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert(False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.040523Z","iopub.status.idle":"2023-04-14T20:07:37.041287Z","shell.execute_reply.started":"2023-04-14T20:07:37.041006Z","shell.execute_reply":"2023-04-14T20:07:37.041031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KARPATHY_DATA['val'][0],KARPATHY_DATA['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.042747Z","iopub.status.idle":"2023-04-14T20:07:37.043655Z","shell.execute_reply.started":"2023-04-14T20:07:37.043282Z","shell.execute_reply":"2023-04-14T20:07:37.043309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef eval_train_split():\n    train_kar=[]\n    train_img=[]\n    for i in KARPATHY_DATA['train']:\n        if i['image'] in train_img:\n            continue\n        train_img.append(i['image'])\n        train_kar.append({'image':i['image'],'caption':[i['caption']]})\n\n    train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config,annot_dict={'val':train_kar,'train':KARPATHY_DATA['train'],'test':KARPATHY_DATA['val']})  \n\n\n    samplers = [None, None, None]\n\n    train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\n                                                          batch_size=[config['batch_size']]*3,num_workers=[4,4,4],\n                                                          is_trains=[True, False, False], collate_fns=[None,None,None])         \n    train_result=evaluate(model, val_loader, device, config)\n    json.dump(train_result,open('/kaggle/working/train_result.json','w'))\n    print(train_result[0])\n    \n    \neval_train_split()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.045162Z","iopub.status.idle":"2023-04-14T20:07:37.045962Z","shell.execute_reply.started":"2023-04-14T20:07:37.045683Z","shell.execute_reply":"2023-04-14T20:07:37.045710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try model generation","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport requests\nimport torch\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimage_size = 384\ndef load_demo_image(image_size,device): \n    raw_image = Image.open(img_url).convert('RGB')   \n\n    w,h = raw_image.size\n    display(raw_image.resize((w//5,h//5)))\n    \n    transform = transforms.Compose([\n        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n        ]) \n    image = transform(raw_image).unsqueeze(0).to(device)   \n    return image\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.047368Z","iopub.status.idle":"2023-04-14T20:07:37.048118Z","shell.execute_reply.started":"2023-04-14T20:07:37.047850Z","shell.execute_reply":"2023-04-14T20:07:37.047887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# beam search\nimg_url=\"/kaggle/input/flickr8k/Images/\"+KARPATHY_DATA['test'][8]['image']\nimage=load_demo_image(image_size,device)\ncaption = model.generate(image, sample=False, num_beams=3, max_length=20, min_length=5) \n# nucleus sampling\n# caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5) \nprint('caption: '+caption[0])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.049465Z","iopub.status.idle":"2023-04-14T20:07:37.050249Z","shell.execute_reply.started":"2023-04-14T20:07:37.049968Z","shell.execute_reply":"2023-04-14T20:07:37.049994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert(False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.051601Z","iopub.status.idle":"2023-04-14T20:07:37.052376Z","shell.execute_reply.started":"2023-04-14T20:07:37.052092Z","shell.execute_reply":"2023-04-14T20:07:37.052118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HappyTT ","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nimport json,string,re\n\nmasked_word=['is','at','for','am','are','and','or','but','of']\npattern=r'\\b('+'|'.join(masked_word)+r')\\b'\n\ndef prepare_tt_kar_text(json_file_path,pattern=None,lower=False):\n    # this function return dictionary \n    with open(json_file_path,'r') as file:\n        data=json.loads(file.read())\n    dict_data=defaultdict(lambda: defaultdict(list))\n    for example in data['images']:\n        temp=[]\n        for sentence in example['sentences']:\n            cap=sentence['raw']\n            if lower:\n                cap=cap.lower()\n                \n            cap=cap.translate(str.maketrans('','',string.punctuation))\n            \n            if pattern is not None:\n                cap=re.sub(pattern,'',cap)\n            cap=' '.join(cap.split())\n            \n            dict_data[example['split']][example['filename']].append(cap)\n\n            \n# {'train':[{'image':'name.jpg','image_id':img_id,'caption':cap}],'test':[{'image':'name.jpg','captions':[cap1,...,cap5]}],'val':[{'image':'name.jpg','captions':[cap1,...,cap5]}]}  \n    return dict_data\nkar_with_pattern=prepare_tt_kar_text('/kaggle/input/karpathy-splits/dataset_flickr8k.json',pattern=pattern,lower=True)\nraw_kar=prepare_tt_kar_text('/kaggle/input/karpathy-splits/dataset_flickr8k.json',pattern=None,lower=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.054093Z","iopub.status.idle":"2023-04-14T20:07:37.054872Z","shell.execute_reply.started":"2023-04-14T20:07:37.054598Z","shell.execute_reply":"2023-04-14T20:07:37.054626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_result=json.load(open('/kaggle/input/blip-mask-results-1e/train_result.json','r'))\n# val_result=json.load(open('/kaggle/input/blip-mask-results-1e/val_result.json','r'))\n# test_result=json.load(open('/kaggle/input/blip-mask-results-1e/test_result.json','r'))\n\n# len(train_result),len(val_result),len(test_result),train_result[0],val_result[0],test_result[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.056241Z","iopub.status.idle":"2023-04-14T20:07:37.056999Z","shell.execute_reply.started":"2023-04-14T20:07:37.056726Z","shell.execute_reply":"2023-04-14T20:07:37.056752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in raw_kar['train']:\n    print(raw_kar['train'][i])\n    print()\n    print(kar_with_pattern['train'][i])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.058351Z","iopub.status.idle":"2023-04-14T20:07:37.059099Z","shell.execute_reply.started":"2023-04-14T20:07:37.058830Z","shell.execute_reply":"2023-04-14T20:07:37.058856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\ncaptioning_results={\n    'train':json.load(open('/kaggle/working/train_result.json','r')),\n    'val':json.load(open('output/Caption_coco/result/val_epoch0.json','r')),\n    'test':json.load(open('output/Caption_coco/result/test_epoch0.json','r'))\n}\ndef generate_csv(csv_path, splits=['train'],input_mode={'pred':True,'masked':True}):\n    # input_dict: {'img_name':['one caption']}\n    # refs_dict: {'img_name':['caption1','cap2','cap3']}\n    \n    with open(csv_path, 'w', newline='') as csvfile:\n        writter = csv.writer(csvfile)\n        writter.writerow([\"input\", \"target\"])\n        for sp in splits:\n            if input_mode['masked']:\n                for img in kar_with_pattern[sp]:\n                    for i in range(len(kar_with_pattern[sp][img])):\n                        writter.writerow(['grammar: '+kar_with_pattern[sp][img][i], raw_kar[sp][img][i]])\n            if input_mode['pred']:\n                for res in captioning_results[sp]:\n                    for cap in raw_kar[sp][res['image_id']]:\n                        writter.writerow(['grammar: '+res['caption'], cap])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.060473Z","iopub.status.idle":"2023-04-14T20:07:37.061236Z","shell.execute_reply.started":"2023-04-14T20:07:37.060963Z","shell.execute_reply":"2023-04-14T20:07:37.060989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ngenerate_csv(\"train.csv\", splits=['train'],input_mode={'pred':True,'masked':True})\ngenerate_csv(\"eval.csv\", splits=['val'],input_mode={'pred':True,'masked':False})\ngenerate_csv(\"test.csv\", splits=['test'],input_mode={'pred':True,'masked':False})","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.062584Z","iopub.status.idle":"2023-04-14T20:07:37.063375Z","shell.execute_reply.started":"2023-04-14T20:07:37.063087Z","shell.execute_reply":"2023-04-14T20:07:37.063114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# df=pd.read_csv('train.csv')\n# len(df)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.064720Z","iopub.status.idle":"2023-04-14T20:07:37.065484Z","shell.execute_reply.started":"2023-04-14T20:07:37.065227Z","shell.execute_reply":"2023-04-14T20:07:37.065254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.066821Z","iopub.status.idle":"2023-04-14T20:07:37.067586Z","shell.execute_reply.started":"2023-04-14T20:07:37.067330Z","shell.execute_reply":"2023-04-14T20:07:37.067356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install happytransformer\n\nfrom happytransformer import HappyTextToText, TTSettings\n\nhappy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n# happy_tt = HappyTextToText(\"T5\", \"t5-base\")\n\n\nargs = TTSettings(num_beams=5, min_length=1)\n\n# Add the prefix \"grammar: \" before each input \nresult = happy_tt.generate_text(\"grammar: This sentences has has bads grammar.\", args=args)\nprint(result.text) # This sentence has bad grammar.","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.068943Z","iopub.status.idle":"2023-04-14T20:07:37.069708Z","shell.execute_reply.started":"2023-04-14T20:07:37.069436Z","shell.execute_reply":"2023-04-14T20:07:37.069463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from happytransformer import TTTrainArgs\nargs2 = TTTrainArgs(batch_size=50)\nhappy_tt.train(\"train.csv\", args=args2)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.071076Z","iopub.status.idle":"2023-04-14T20:07:37.071843Z","shell.execute_reply.started":"2023-04-14T20:07:37.071578Z","shell.execute_reply":"2023-04-14T20:07:37.071606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# saving model","metadata":{}},{"cell_type":"code","source":"happy_tt.save(\"/kaggle/working/happy_tt/\")","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.073218Z","iopub.status.idle":"2023-04-14T20:07:37.073978Z","shell.execute_reply.started":"2023-04-14T20:07:37.073705Z","shell.execute_reply":"2023-04-14T20:07:37.073732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"before_training=happy_tt.eval('eval.csv')\nprint(before_training.loss)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.075342Z","iopub.status.idle":"2023-04-14T20:07:37.076101Z","shell.execute_reply.started":"2023-04-14T20:07:37.075820Z","shell.execute_reply":"2023-04-14T20:07:37.075847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds2=[]\nannot={'annotations':[],'images':[]}\nfor i,res in enumerate(tqdm(captioning_results['test'])):\n    out_cap=happy_tt.generate_text(\"grammar: \"+res['caption'], args=args).text\n    preds2.append({'image_id':res['image_id'],'caption':out_cap})\n    for cap in raw_kar['test'][res['image_id']]:\n        annot['images'].append({'id':res['image_id']})\n        annot['annotations'].append({'image_id':res['image_id'],'caption':cap,'id':i})\n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.077466Z","iopub.status.idle":"2023-04-14T20:07:37.078240Z","shell.execute_reply.started":"2023-04-14T20:07:37.077957Z","shell.execute_reply":"2023-04-14T20:07:37.077982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"json.dump(annot,open('/kaggle/working/coco_karpathy_test_gt.json','w'))\njson.dump(preds2,open('/kaggle/working/coco_karpathy_test_result.json','w'))\ncoco_val = coco_caption_eval('/kaggle/working','/kaggle/working/coco_karpathy_test_result.json','test')","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.079602Z","iopub.status.idle":"2023-04-14T20:07:37.080384Z","shell.execute_reply.started":"2023-04-14T20:07:37.080100Z","shell.execute_reply":"2023-04-14T20:07:37.080128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert(False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.081727Z","iopub.status.idle":"2023-04-14T20:07:37.082498Z","shell.execute_reply.started":"2023-04-14T20:07:37.082240Z","shell.execute_reply":"2023-04-14T20:07:37.082268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"json.dump(annot,open('/kaggle/working/coco_karpathy_test_gt.json','w'))\njson.dump(preds2,open('/kaggle/working/coco_karpathy_test_result.json','w'))\ncoco_val = coco_caption_eval('/kaggle/working','/kaggle/working/coco_karpathy_test_result.json','test')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T20:07:37.083819Z","iopub.status.idle":"2023-04-14T20:07:37.084585Z","shell.execute_reply.started":"2023-04-14T20:07:37.084329Z","shell.execute_reply":"2023-04-14T20:07:37.084355Z"},"trusted":true},"execution_count":null,"outputs":[]}]}